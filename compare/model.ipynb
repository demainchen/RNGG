{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, residual=False, variant=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.variant = variant\n",
    "        if self.variant:\n",
    "            self.in_features = 2*in_features\n",
    "        else:\n",
    "            self.in_features = in_features\n",
    "\n",
    "        self.out_features = out_features\n",
    "        self.residual = residual\n",
    "        self.weight = Parameter(torch.FloatTensor(self.in_features,self.out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj , h0 , lamda, alpha, l):\n",
    "        theta = min(1, math.log(lamda/l+1))\n",
    "        hi = torch.spmm(adj, input)\n",
    "        if self.variant:\n",
    "            support = torch.cat([hi,h0],1)\n",
    "            r = (1-alpha)*hi+alpha*h0\n",
    "        else:\n",
    "            support = (1-alpha)*hi+alpha*h0\n",
    "            r = support\n",
    "        output = theta*torch.mm(support, self.weight)+(1-theta)*r\n",
    "        if self.residual: # speed up convergence of the training process\n",
    "            output = output+input\n",
    "        return output\n",
    "\n",
    "\n",
    "class deepGCN(nn.Module):\n",
    "    def __init__(self, nlayers, nfeat, nhidden, nclass, dropout, lamda, alpha, variant):\n",
    "        super(deepGCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(nlayers):\n",
    "            self.convs.append(GraphConvolution(nhidden, nhidden,variant=variant,residual=True))\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(nfeat, nhidden))\n",
    "        self.fcs.append(nn.Linear(nhidden, nclass))\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        _layers = []\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        layer_inner = self.act_fn(self.fcs[0](x))\n",
    "        _layers.append(layer_inner)\n",
    "        for i,con in enumerate(self.convs):\n",
    "            layer_inner = F.dropout(layer_inner, self.dropout, training=self.training)\n",
    "            layer_inner = self.act_fn(con(layer_inner,adj,_layers[0],self.lamda,self.alpha,i+1))\n",
    "        layer_inner = F.dropout(layer_inner, self.dropout, training=self.training)\n",
    "        layer_inner = self.fcs[-1](layer_inner)\n",
    "        return layer_inner\n",
    "\n",
    "\n",
    "class GraphPPIS(nn.Module):\n",
    "    def __init__(self, nlayers, nfeat, nhidden, nclass, dropout, lamda, alpha, variant):\n",
    "        super(GraphPPIS, self).__init__()\n",
    "\n",
    "        self.deep_gcn = deepGCN(nlayers = nlayers, nfeat = nfeat, nhidden = nhidden, nclass = nclass,\n",
    "                                dropout = dropout, lamda = lamda, alpha = alpha, variant = variant)\n",
    "        self.criterion = nn.CrossEntropyLoss() # automatically do softmax to the predicted value and one-hot to the label\n",
    "        # self.optimizer = torch.optim.Adam(self.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "    def forward(self, x, adj):          # x.shape = (seq_len, FEATURE_DIM); adj.shape = (seq_len, seq_len)\n",
    "        x = x.float()\n",
    "        output = self.deep_gcn(x, adj)  # output.shape = (seq_len, NUM_CLASSES)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAP_CUTOFF = 14\n",
    "HIDDEN_DIM = 256\n",
    "LAYER = 8\n",
    "DROPOUT = 0.1\n",
    "ALPHA = 0.7\n",
    "LAMBDA = 1.5\n",
    "VARIANT = True # From GCNII\n",
    "\n",
    "LEARNING_RATE = 1E-3\n",
    "WEIGHT_DECAY = 0\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASSES = 2 # [not bind, bind]\n",
    "\n",
    "INPUT_DIM=54\n",
    "model=GraphPPIS(LAYER, INPUT_DIM, HIDDEN_DIM, NUM_CLASSES, DROPOUT, LAMBDA, ALPHA, VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('GraphPPIS_slow.pkl',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphPPIS(\n",
       "  (deep_gcn): deepGCN(\n",
       "    (convs): ModuleList(\n",
       "      (0): GraphConvolution()\n",
       "      (1): GraphConvolution()\n",
       "      (2): GraphConvolution()\n",
       "      (3): GraphConvolution()\n",
       "      (4): GraphConvolution()\n",
       "      (5): GraphConvolution()\n",
       "      (6): GraphConvolution()\n",
       "      (7): GraphConvolution()\n",
       "    )\n",
       "    (fcs): ModuleList(\n",
       "      (0): Linear(in_features=54, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=2, bias=True)\n",
       "    )\n",
       "    (act_fn): ReLU()\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 100\n",
      "lr 0.001\n",
      "train_data_path ../数据集/train_335.pkl\n",
      "test_data_path ../数据集/Test_60.pkl\n",
      "test_315 ../数据集/Test_315.pkl\n",
      "seed 10\n",
      "split_rate 0.2\n",
      "batch_size 1\n",
      "save_path ../result/\n",
      "device cpu\n",
      "loss_fun CrossEntropyLoss()\n",
      "Threashold 0.2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.train_data_path = '../数据集/train_335.pkl'\n",
    "        self.test_data_path = '../数据集/Test_60.pkl'\n",
    "        self.test_315= '../数据集/Test_315.pkl'\n",
    "        self.seed = 10\n",
    "        self.split_rate = 0.2\n",
    "        self.batch_size = 1\n",
    "        self.save_path = '../result/'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.loss_fun = nn.CrossEntropyLoss().to(self.device)\n",
    "        self.Threashold = 0.2\n",
    "\n",
    "        for name, value in vars(self).items():\n",
    "            print(name, value)\n",
    "\n",
    "        self.save_txt = 'result.txt'\n",
    "\n",
    "        # GN\n",
    "        self.MAP_CUTOFF = 14\n",
    "        self.HIDDEN_DIM = 256\n",
    "        self.LAYER = 8\n",
    "        self.DROPOUT = 0.1\n",
    "        self.ALPHA = 0.7\n",
    "        self.LAMBDA = 1.5\n",
    "        self.VARIANT = True  # From GCNII\n",
    "\n",
    "        self.WEIGHT_DECAY = 0\n",
    "        self.BATCH_SIZE = 1\n",
    "        self.NUM_CLASSES = 1  # [not bind, bind]\n",
    "        self.INPUT_DIM = 54\n",
    "\n",
    "        self.negative_slope = 0.3\n",
    "        # self.leaky_relu=nn.LeakyReLU(self.negative_slope)\n",
    "        self.leaky_relu = nn.ReLU()\n",
    "\n",
    "        self.drop_layer = nn.Dropout(0.3)\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(32).to(self.device)\n",
    "config=Config()\n",
    "def normalize(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = (rowsum ** (-0.5)).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0\n",
    "    r_inv[np.isnan(r_inv)] = 0\n",
    "    r_mat_inv = np.diag(r_inv)\n",
    "    result = r_mat_inv @ mx @ r_mat_inv\n",
    "    return result\n",
    "\n",
    "class Test_Data(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        df = open(data_path, 'rb')\n",
    "        self.raw_data = pickle.load(df)\n",
    "        self.protein_list = list(self.raw_data.keys())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        protein_name = self.protein_list[index]\n",
    "        protein_inf = self.raw_data[protein_name]\n",
    "        labels = torch.tensor(np.array([float(i) for i in protein_inf['label']]), requires_grad=True).float().to(\n",
    "            config.device)\n",
    "        seq_emb = torch.tensor(np.squeeze(protein_inf['seq_emb']), requires_grad=True).squeeze().to(config.device)\n",
    "        structure_emb = torch.tensor(np.squeeze(normalize(protein_inf['s2'])),\n",
    "                                     requires_grad=True).squeeze().to(config.device)\n",
    "\n",
    "        dssp = torch.tensor(np.squeeze(protein_inf['dssp']), requires_grad=True).to(config.device)\n",
    "        hmm = torch.tensor(np.squeeze(protein_inf['hmm']), requires_grad=True).to(config.device)\n",
    "        pssm = torch.tensor(np.squeeze(protein_inf['pssm']), requires_grad=True).to(config.device)\n",
    "        return dssp, hmm, pssm, seq_emb, structure_emb, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.protein_list)\n",
    "\n",
    "test_data = Test_Data(data_path=config.test_315)\n",
    "eval_data_loader = DataLoader(dataset=test_data, batch_size=config.batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def eval(eval_data_loader, model, epoch):\n",
    "    model.eval()\n",
    "    pred=[]\n",
    "    label=[]\n",
    "    for i, (dssp, hmm, pssm, seq_emb, structure_emb, labels) in enumerate(eval_data_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        seq_emb = seq_emb.squeeze().to(config.device)\n",
    "        structure_emb = structure_emb.squeeze().to(config.device)\n",
    "        labels = labels.squeeze().unsqueeze(dim=-1).to(config.device)\n",
    "        dssp = dssp.squeeze().to(config.device)\n",
    "        hmm = hmm.squeeze().to(config.device)\n",
    "        pssm = pssm.squeeze().to(config.device)\n",
    "\n",
    "        node_features=torch.cat((pssm,hmm,dssp),dim=1).to(torch.float)\n",
    "        structure_emb=structure_emb.to(torch.float)\n",
    "        y_pred = model(node_features, structure_emb)\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        y_pred = softmax(y_pred)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "        pred += [pred[1] for pred in y_pred]\n",
    "        label+=[float(l) for l in labels]\n",
    "    return pred,label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred,label=eval(eval_data_loader,model,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13942364,\n",
       " 0.3876789,\n",
       " 0.063507825,\n",
       " 0.23615764,\n",
       " 0.060792625,\n",
       " 0.15969332,\n",
       " 0.037329253,\n",
       " 0.0739717,\n",
       " 0.062388223,\n",
       " 0.12879464,\n",
       " 0.10886415,\n",
       " 0.055108417,\n",
       " 0.19469222,\n",
       " 0.057752706,\n",
       " 0.028229969,\n",
       " 0.10543079,\n",
       " 0.36642578,\n",
       " 0.03391277,\n",
       " 0.10819287,\n",
       " 0.36907944,\n",
       " 0.3341548,\n",
       " 0.5186997,\n",
       " 0.4633626,\n",
       " 0.5162193,\n",
       " 0.55679697,\n",
       " 0.5041631,\n",
       " 0.5240414,\n",
       " 0.2852365,\n",
       " 0.2505785,\n",
       " 0.083142385,\n",
       " 0.103625216,\n",
       " 0.09379938,\n",
       " 0.14040156,\n",
       " 0.045036487,\n",
       " 0.1905177,\n",
       " 0.11467462,\n",
       " 0.40390736,\n",
       " 0.14410408,\n",
       " 0.6501423,\n",
       " 0.48478803,\n",
       " 0.5248542,\n",
       " 0.2295889,\n",
       " 0.42392522,\n",
       " 0.28451908,\n",
       " 0.15427381,\n",
       " 0.11120022,\n",
       " 0.11332603,\n",
       " 0.06662278,\n",
       " 0.3166683,\n",
       " 0.12424417,\n",
       " 0.11223361,\n",
       " 0.098574415,\n",
       " 0.33569202,\n",
       " 0.0651864,\n",
       " 0.2047081,\n",
       " 0.15535411,\n",
       " 0.24869001,\n",
       " 0.29835194,\n",
       " 0.2969261,\n",
       " 0.14491779,\n",
       " 0.3321119,\n",
       " 0.33989367,\n",
       " 0.43251947,\n",
       " 0.31797087,\n",
       " 0.40233436,\n",
       " 0.4894945,\n",
       " 0.36346823,\n",
       " 0.3159883,\n",
       " 0.5669781,\n",
       " 0.52583396,\n",
       " 0.38522,\n",
       " 0.40522444,\n",
       " 0.22555676,\n",
       " 0.1405046,\n",
       " 0.6266225,\n",
       " 0.14592354,\n",
       " 0.03730271,\n",
       " 0.15115939,\n",
       " 0.100419246,\n",
       " 0.071577825,\n",
       " 0.025824716,\n",
       " 0.013343668,\n",
       " 0.057361618,\n",
       " 0.039796285,\n",
       " 0.03239738,\n",
       " 0.13079472,\n",
       " 0.12933028,\n",
       " 0.06860367,\n",
       " 0.06232599,\n",
       " 0.322108,\n",
       " 0.14589012,\n",
       " 0.02576006,\n",
       " 0.060159232,\n",
       " 0.1700721,\n",
       " 0.07669454,\n",
       " 0.16509634,\n",
       " 0.05828116,\n",
       " 0.12130896,\n",
       " 0.11343403,\n",
       " 0.07811242,\n",
       " 0.106895685,\n",
       " 0.06985748,\n",
       " 0.026116828,\n",
       " 0.030922761,\n",
       " 0.044678114,\n",
       " 0.05316145,\n",
       " 0.049100224,\n",
       " 0.015811525,\n",
       " 0.021471908,\n",
       " 0.009503849,\n",
       " 0.069762595,\n",
       " 0.011281743,\n",
       " 0.09452392,\n",
       " 0.013756571,\n",
       " 0.14904624,\n",
       " 0.13955067,\n",
       " 0.015522568,\n",
       " 0.14866813,\n",
       " 0.051076494,\n",
       " 0.115614444,\n",
       " 0.10841427,\n",
       " 0.02804633,\n",
       " 0.035394855,\n",
       " 0.039924398,\n",
       " 0.05331587,\n",
       " 0.007243814,\n",
       " 0.035357825,\n",
       " 0.04972042,\n",
       " 0.012209047,\n",
       " 0.009314442,\n",
       " 0.02356872,\n",
       " 0.036852427,\n",
       " 0.05869876,\n",
       " 0.042936876,\n",
       " 0.021887787,\n",
       " 0.028754208,\n",
       " 0.0063241613,\n",
       " 0.03537358,\n",
       " 0.037232086,\n",
       " 0.063002974,\n",
       " 0.11315202,\n",
       " 0.28267735,\n",
       " 0.19491643,\n",
       " 0.5013676,\n",
       " 0.3085123,\n",
       " 0.25840333,\n",
       " 0.23263074,\n",
       " 0.3453713,\n",
       " 0.23966007,\n",
       " 0.27228558,\n",
       " 0.34685728,\n",
       " 0.20786358,\n",
       " 0.18256679,\n",
       " 0.31019682,\n",
       " 0.27196887,\n",
       " 0.23070891,\n",
       " 0.07894489,\n",
       " 0.092211746,\n",
       " 0.0045890016,\n",
       " 0.070544876,\n",
       " 0.008649304,\n",
       " 0.051298633,\n",
       " 0.018566811,\n",
       " 0.043139666,\n",
       " 0.034364052,\n",
       " 0.08329571,\n",
       " 0.03269555,\n",
       " 0.09394944,\n",
       " 0.06576855,\n",
       " 0.007534106,\n",
       " 0.02002144,\n",
       " 0.09969186,\n",
       " 0.04013817,\n",
       " 0.009388261,\n",
       " 0.033039346,\n",
       " 0.03336197,\n",
       " 0.014548112,\n",
       " 0.029555786,\n",
       " 0.06403965,\n",
       " 0.04172347,\n",
       " 0.06554236,\n",
       " 0.18849325,\n",
       " 0.31479904,\n",
       " 0.27039927,\n",
       " 0.1518358,\n",
       " 0.49875996,\n",
       " 0.393024,\n",
       " 0.1960118,\n",
       " 0.23488519,\n",
       " 0.17163298,\n",
       " 0.19575125,\n",
       " 0.18461765,\n",
       " 0.27309248,\n",
       " 0.14228116,\n",
       " 0.28421155,\n",
       " 0.3263449,\n",
       " 0.1113504,\n",
       " 0.2069443,\n",
       " 0.19151719,\n",
       " 0.23428442,\n",
       " 0.036767658,\n",
       " 0.16406974,\n",
       " 0.18137784,\n",
       " 0.28794062,\n",
       " 0.09660494,\n",
       " 0.17157221,\n",
       " 0.22089235,\n",
       " 0.17221935,\n",
       " 0.23289779,\n",
       " 0.15047224,\n",
       " 0.281849,\n",
       " 0.26266435,\n",
       " 0.1391093,\n",
       " 0.07014087,\n",
       " 0.46616578,\n",
       " 0.39405206,\n",
       " 0.45173123,\n",
       " 0.37157005,\n",
       " 0.21077627,\n",
       " 0.4340784,\n",
       " 0.07423943,\n",
       " 0.4033457,\n",
       " 0.50916386,\n",
       " 0.18779872,\n",
       " 0.34943068,\n",
       " 0.62726367,\n",
       " 0.49111655,\n",
       " 0.40074554,\n",
       " 0.13606213,\n",
       " 0.26415455,\n",
       " 0.11192016,\n",
       " 0.06375043,\n",
       " 0.14368498,\n",
       " 0.40773037,\n",
       " 0.28212243,\n",
       " 0.1920616,\n",
       " 0.23932534,\n",
       " 0.10293612,\n",
       " 0.11941947,\n",
       " 0.16457117,\n",
       " 0.14217007,\n",
       " 0.15744251,\n",
       " 0.14766681,\n",
       " 0.17275831,\n",
       " 0.17213047,\n",
       " 0.1536002,\n",
       " 0.16004458,\n",
       " 0.15193339,\n",
       " 0.15978082,\n",
       " 0.23625664,\n",
       " 0.18949594,\n",
       " 0.4016796,\n",
       " 0.30686393,\n",
       " 0.42803448,\n",
       " 0.24632113,\n",
       " 0.47532308,\n",
       " 0.20811602,\n",
       " 0.39622265,\n",
       " 0.28547397,\n",
       " 0.07744983,\n",
       " 0.10480461,\n",
       " 0.2083005,\n",
       " 0.22375673,\n",
       " 0.07029167,\n",
       " 0.044938847,\n",
       " 0.06216435,\n",
       " 0.20970088,\n",
       " 0.024248509,\n",
       " 0.08570076,\n",
       " 0.10750375,\n",
       " 0.028494967,\n",
       " 0.11290014,\n",
       " 0.07752706,\n",
       " 0.079566754,\n",
       " 0.04677028,\n",
       " 0.08418405,\n",
       " 0.14898537,\n",
       " 0.10127349,\n",
       " 0.17417446,\n",
       " 0.16431493,\n",
       " 0.12165418,\n",
       " 0.09230076,\n",
       " 0.027149223,\n",
       " 0.050439324,\n",
       " 0.061227422,\n",
       " 0.038872287,\n",
       " 0.048816588,\n",
       " 0.04198598,\n",
       " 0.049080756,\n",
       " 0.04381929,\n",
       " 0.035677854,\n",
       " 0.06395299,\n",
       " 0.041080657,\n",
       " 0.052798066,\n",
       " 0.08985151,\n",
       " 0.08541429,\n",
       " 0.07759721,\n",
       " 0.09320575,\n",
       " 0.068930395,\n",
       " 0.064990185,\n",
       " 0.01979848,\n",
       " 0.24654962,\n",
       " 0.16076986,\n",
       " 0.025391353,\n",
       " 0.07906326,\n",
       " 0.24798934,\n",
       " 0.15880959,\n",
       " 0.15145583,\n",
       " 0.21130864,\n",
       " 0.11032766,\n",
       " 0.22103226,\n",
       " 0.18657218,\n",
       " 0.078728564,\n",
       " 0.16635507,\n",
       " 0.075210564,\n",
       " 0.10978285,\n",
       " 0.0807846,\n",
       " 0.05238183,\n",
       " 0.07059197,\n",
       " 0.06035805,\n",
       " 0.07134982,\n",
       " 0.0703341,\n",
       " 0.15733507,\n",
       " 0.19137357,\n",
       " 0.21951564,\n",
       " 0.13272765,\n",
       " 0.28289172,\n",
       " 0.23269607,\n",
       " 0.19182684,\n",
       " 0.6234911,\n",
       " 0.4559017,\n",
       " 0.40740305,\n",
       " 0.39392427,\n",
       " 0.5540952,\n",
       " 0.25929004,\n",
       " 0.15906034,\n",
       " 0.1020534,\n",
       " 0.09773382,\n",
       " 0.05742097,\n",
       " 0.06640691,\n",
       " 0.31639573,\n",
       " 0.36893317,\n",
       " 0.40568215,\n",
       " 0.38247812,\n",
       " 0.42409384,\n",
       " 0.33881402,\n",
       " 0.26815522,\n",
       " 0.37033728,\n",
       " 0.28271088,\n",
       " 0.109177954,\n",
       " 0.03322535,\n",
       " 0.049238138,\n",
       " 0.043853328,\n",
       " 0.03645745,\n",
       " 0.23970167,\n",
       " 0.059622634,\n",
       " 0.31520998,\n",
       " 0.0539869,\n",
       " 0.16179727,\n",
       " 0.07970999,\n",
       " 0.18272185,\n",
       " 0.520077,\n",
       " 0.18969798,\n",
       " 0.1314084,\n",
       " 0.3517185,\n",
       " 0.34102827,\n",
       " 0.16335355,\n",
       " 0.14567915,\n",
       " 0.17251773,\n",
       " 0.3084303,\n",
       " 0.22400914,\n",
       " 0.072502755,\n",
       " 0.11571386,\n",
       " 0.51305825,\n",
       " 0.09220342,\n",
       " 0.03926945,\n",
       " 0.15311499,\n",
       " 0.29539296,\n",
       " 0.044411417,\n",
       " 0.01955101,\n",
       " 0.32124543,\n",
       " 0.12455738,\n",
       " 0.061125297,\n",
       " 0.012564901,\n",
       " 0.08940355,\n",
       " 0.06539686,\n",
       " 0.061606415,\n",
       " 0.027345348,\n",
       " 0.07559897,\n",
       " 0.062294714,\n",
       " 0.04862976,\n",
       " 0.04030189,\n",
       " 0.021313246,\n",
       " 0.15390804,\n",
       " 0.23990369,\n",
       " 0.34089705,\n",
       " 0.45627126,\n",
       " 0.6360126,\n",
       " 0.6516514,\n",
       " 0.6378382,\n",
       " 0.38942885,\n",
       " 0.29885176,\n",
       " 0.111698814,\n",
       " 0.09892041,\n",
       " 0.039560426,\n",
       " 0.045582876,\n",
       " 0.021935422,\n",
       " 0.049639575,\n",
       " 0.150943,\n",
       " 0.12729816,\n",
       " 0.08053856,\n",
       " 0.116333604,\n",
       " 0.09171701,\n",
       " 0.07259868,\n",
       " 0.070804626,\n",
       " 0.034514673,\n",
       " 0.023132812,\n",
       " 0.04514802,\n",
       " 0.05176377,\n",
       " 0.010717109,\n",
       " 0.008008386,\n",
       " 0.060053322,\n",
       " 0.0238042,\n",
       " 0.015205335,\n",
       " 0.04607771,\n",
       " 0.056163706,\n",
       " 0.013860696,\n",
       " 0.07602365,\n",
       " 0.061867878,\n",
       " 0.04380547,\n",
       " 0.017801762,\n",
       " 0.057020437,\n",
       " 0.035898563,\n",
       " 0.08270923,\n",
       " 0.07611629,\n",
       " 0.057423994,\n",
       " 0.051580697,\n",
       " 0.09961604,\n",
       " 0.019988524,\n",
       " 0.037159894,\n",
       " 0.029006692,\n",
       " 0.040725227,\n",
       " 0.074019946,\n",
       " 0.02075697,\n",
       " 0.15284951,\n",
       " 0.019120513,\n",
       " 0.010651396,\n",
       " 0.010446386,\n",
       " 0.08196856,\n",
       " 0.09211252,\n",
       " 0.13124844,\n",
       " 0.2463366,\n",
       " 0.08441676,\n",
       " 0.47722787,\n",
       " 0.23479111,\n",
       " 0.37219223,\n",
       " 0.38087103,\n",
       " 0.5167767,\n",
       " 0.29368263,\n",
       " 0.32039136,\n",
       " 0.21567748,\n",
       " 0.21145302,\n",
       " 0.37158868,\n",
       " 0.34833542,\n",
       " 0.51504755,\n",
       " 0.40340206,\n",
       " 0.667506,\n",
       " 0.5579506,\n",
       " 0.31812564,\n",
       " 0.22041099,\n",
       " 0.21568988,\n",
       " 0.43222153,\n",
       " 0.13349728,\n",
       " 0.057885494,\n",
       " 0.40940547,\n",
       " 0.31531486,\n",
       " 0.022366121,\n",
       " 0.110711105,\n",
       " 0.19636397,\n",
       " 0.1416227,\n",
       " 0.08306487,\n",
       " 0.12450345,\n",
       " 0.07100466,\n",
       " 0.08676471,\n",
       " 0.11178039,\n",
       " 0.026355805,\n",
       " 0.050538942,\n",
       " 0.014880507,\n",
       " 0.02558632,\n",
       " 0.047657765,\n",
       " 0.04842115,\n",
       " 0.009286193,\n",
       " 0.10711128,\n",
       " 0.086103864,\n",
       " 0.067101724,\n",
       " 0.016152743,\n",
       " 0.07903176,\n",
       " 0.12055438,\n",
       " 0.10078511,\n",
       " 0.12433158,\n",
       " 0.09189246,\n",
       " 0.07427984,\n",
       " 0.12181976,\n",
       " 0.116745286,\n",
       " 0.18030144,\n",
       " 0.114572495,\n",
       " 0.10302668,\n",
       " 0.10652701,\n",
       " 0.12684241,\n",
       " 0.18644772,\n",
       " 0.10326302,\n",
       " 0.11214113,\n",
       " 0.061936744,\n",
       " 0.122881286,\n",
       " 0.16913034,\n",
       " 0.12587667,\n",
       " 0.10926277,\n",
       " 0.14963031,\n",
       " 0.16751763,\n",
       " 0.14557569,\n",
       " 0.12721351,\n",
       " 0.09636205,\n",
       " 0.0927482,\n",
       " 0.029067412,\n",
       " 0.11189215,\n",
       " 0.104657434,\n",
       " 0.026883123,\n",
       " 0.106384225,\n",
       " 0.1404105,\n",
       " 0.23037969,\n",
       " 0.20084056,\n",
       " 0.026236461,\n",
       " 0.044097535,\n",
       " 0.048987105,\n",
       " 0.060546678,\n",
       " 0.07115311,\n",
       " 0.040459633,\n",
       " 0.047947068,\n",
       " 0.086982585,\n",
       " 0.07523248,\n",
       " 0.08190855,\n",
       " 0.11668781,\n",
       " 0.11782539,\n",
       " 0.07564285,\n",
       " 0.047288787,\n",
       " 0.08174747,\n",
       " 0.09715589,\n",
       " 0.01941364,\n",
       " 0.08190921,\n",
       " 0.08228587,\n",
       " 0.04855853,\n",
       " 0.021575524,\n",
       " 0.07031694,\n",
       " 0.04888788,\n",
       " 0.04817539,\n",
       " 0.06534112,\n",
       " 0.039097905,\n",
       " 0.12326688,\n",
       " 0.09640794,\n",
       " 0.102079496,\n",
       " 0.095520355,\n",
       " 0.11798907,\n",
       " 0.06877666,\n",
       " 0.05217914,\n",
       " 0.056667026,\n",
       " 0.048173293,\n",
       " 0.055719394,\n",
       " 0.01406573,\n",
       " 0.01117232,\n",
       " 0.034545638,\n",
       " 0.025964845,\n",
       " 0.021841096,\n",
       " 0.10834324,\n",
       " 0.1368127,\n",
       " 0.14216557,\n",
       " 0.1411328,\n",
       " 0.15155368,\n",
       " 0.022551335,\n",
       " 0.021214614,\n",
       " 0.01166736,\n",
       " 0.043483812,\n",
       " 0.03861233,\n",
       " 0.059898075,\n",
       " 0.1841659,\n",
       " 0.10270215,\n",
       " 0.112020925,\n",
       " 0.098739766,\n",
       " 0.18605585,\n",
       " 0.08312804,\n",
       " 0.022866886,\n",
       " 0.02289634,\n",
       " 0.026701028,\n",
       " 0.028776303,\n",
       " 0.014425518,\n",
       " 0.02696254,\n",
       " 0.023897352,\n",
       " 0.03408945,\n",
       " 0.071596965,\n",
       " 0.076571584,\n",
       " 0.12351688,\n",
       " 0.030512264,\n",
       " 0.054121543,\n",
       " 0.03979729,\n",
       " 0.026329357,\n",
       " 0.03343437,\n",
       " 0.03762471,\n",
       " 0.12928903,\n",
       " 0.27472124,\n",
       " 0.36438778,\n",
       " 0.3645764,\n",
       " 0.24827985,\n",
       " 0.31190678,\n",
       " 0.06851862,\n",
       " 0.15068099,\n",
       " 0.18412171,\n",
       " 0.17789385,\n",
       " 0.086931475,\n",
       " 0.070099205,\n",
       " 0.11708143,\n",
       " 0.011357661,\n",
       " 0.086490914,\n",
       " 0.043872263,\n",
       " 0.0518132,\n",
       " 0.06666392,\n",
       " 0.03711728,\n",
       " 0.07506824,\n",
       " 0.0748091,\n",
       " 0.087659165,\n",
       " 0.07188915,\n",
       " 0.036661815,\n",
       " 0.064424396,\n",
       " 0.07513815,\n",
       " 0.082695685,\n",
       " 0.08936583,\n",
       " 0.079444766,\n",
       " 0.06253156,\n",
       " 0.048097413,\n",
       " 0.060139533,\n",
       " 0.042876594,\n",
       " 0.035670184,\n",
       " 0.0519793,\n",
       " 0.030319005,\n",
       " 0.025278976,\n",
       " 0.033849683,\n",
       " 0.04100148,\n",
       " 0.01586569,\n",
       " 0.039306812,\n",
       " 0.040887978,\n",
       " 0.035308376,\n",
       " 0.065487474,\n",
       " 0.041982282,\n",
       " 0.08546295,\n",
       " 0.033487402,\n",
       " 0.069280535,\n",
       " 0.020716883,\n",
       " 0.023814468,\n",
       " 0.060609583,\n",
       " 0.047251623,\n",
       " 0.06378754,\n",
       " 0.057641357,\n",
       " 0.12784982,\n",
       " 0.0600002,\n",
       " 0.040780067,\n",
       " 0.07015377,\n",
       " 0.07196207,\n",
       " 0.05636598,\n",
       " 0.058275774,\n",
       " 0.032068763,\n",
       " 0.014424624,\n",
       " 0.041348476,\n",
       " 0.03606898,\n",
       " 0.03801491,\n",
       " 0.01822374,\n",
       " 0.089630336,\n",
       " 0.056945816,\n",
       " 0.028765144,\n",
       " 0.14416556,\n",
       " 0.1029669,\n",
       " 0.11759856,\n",
       " 0.07938526,\n",
       " 0.074517265,\n",
       " 0.050535105,\n",
       " 0.09803416,\n",
       " 0.0800967,\n",
       " 0.073598996,\n",
       " 0.03131049,\n",
       " 0.049774576,\n",
       " 0.24038741,\n",
       " 0.33246887,\n",
       " 0.017862545,\n",
       " 0.1344917,\n",
       " 0.13789038,\n",
       " 0.03659248,\n",
       " 0.11310274,\n",
       " 0.17276369,\n",
       " 0.33775678,\n",
       " 0.3022248,\n",
       " 0.51684624,\n",
       " 0.43118703,\n",
       " 0.517266,\n",
       " 0.41849586,\n",
       " 0.20737776,\n",
       " 0.18159929,\n",
       " 0.10930881,\n",
       " 0.14764766,\n",
       " 0.08827284,\n",
       " 0.0537468,\n",
       " 0.09834382,\n",
       " 0.085217714,\n",
       " 0.052004155,\n",
       " 0.18020593,\n",
       " 0.15392199,\n",
       " 0.12200062,\n",
       " 0.08838738,\n",
       " 0.18536901,\n",
       " 0.18395686,\n",
       " 0.18671834,\n",
       " 0.08046279,\n",
       " 0.15483207,\n",
       " 0.025390875,\n",
       " 0.02163664,\n",
       " 0.026230844,\n",
       " 0.099511646,\n",
       " 0.09749119,\n",
       " 0.1179038,\n",
       " 0.07033952,\n",
       " 0.12458511,\n",
       " 0.12310571,\n",
       " 0.18975922,\n",
       " 0.113691404,\n",
       " 0.118272215,\n",
       " 0.070064776,\n",
       " 0.14003633,\n",
       " 0.03438036,\n",
       " 0.14696667,\n",
       " 0.10183215,\n",
       " 0.12777917,\n",
       " 0.055635788,\n",
       " 0.08128439,\n",
       " 0.058309097,\n",
       " 0.13975368,\n",
       " 0.05721333,\n",
       " 0.051992025,\n",
       " 0.05032293,\n",
       " 0.05306746,\n",
       " 0.025250081,\n",
       " 0.050658606,\n",
       " 0.06383591,\n",
       " 0.071428314,\n",
       " 0.07002268,\n",
       " 0.05834139,\n",
       " 0.046506878,\n",
       " 0.05542975,\n",
       " 0.06374309,\n",
       " 0.03458364,\n",
       " 0.10554057,\n",
       " 0.06518398,\n",
       " 0.12219345,\n",
       " 0.15058438,\n",
       " 0.17840038,\n",
       " 0.16252846,\n",
       " 0.086254865,\n",
       " 0.15077981,\n",
       " 0.11445772,\n",
       " 0.040016204,\n",
       " 0.07908967,\n",
       " 0.059147805,\n",
       " 0.03851075,\n",
       " 0.019681243,\n",
       " 0.049523085,\n",
       " 0.007534658,\n",
       " 0.012737079,\n",
       " 0.027040683,\n",
       " 0.060791302,\n",
       " 0.0076797507,\n",
       " 0.03853272,\n",
       " 0.060950696,\n",
       " 0.066838615,\n",
       " 0.03729917,\n",
       " 0.03416922,\n",
       " 0.052295323,\n",
       " 0.043173764,\n",
       " 0.022695076,\n",
       " 0.005222481,\n",
       " 0.050235506,\n",
       " 0.020268066,\n",
       " 0.21711981,\n",
       " 0.06980555,\n",
       " 0.43592137,\n",
       " 0.5363749,\n",
       " 0.48947072,\n",
       " 0.41315463,\n",
       " 0.23017578,\n",
       " 0.07320395,\n",
       " 0.20662633,\n",
       " 0.12177215,\n",
       " 0.019779066,\n",
       " 0.039813805,\n",
       " 0.031513393,\n",
       " 0.02495907,\n",
       " 0.0041899523,\n",
       " 0.022847423,\n",
       " 0.037862815,\n",
       " 0.026219277,\n",
       " 0.018672002,\n",
       " 0.0063779536,\n",
       " 0.018726801,\n",
       " 0.008857241,\n",
       " 0.052306164,\n",
       " 0.124129094,\n",
       " 0.15512808,\n",
       " 0.31441697,\n",
       " 0.3609387,\n",
       " 0.30008692,\n",
       " 0.43043104,\n",
       " 0.18209532,\n",
       " 0.19552861,\n",
       " 0.100481585,\n",
       " 0.08354155,\n",
       " 0.3502198,\n",
       " 0.11727282,\n",
       " 0.057342567,\n",
       " 0.079333894,\n",
       " 0.17953056,\n",
       " 0.03451999,\n",
       " 0.03303977,\n",
       " 0.018633857,\n",
       " 0.057068292,\n",
       " 0.020257208,\n",
       " 0.023293719,\n",
       " 0.024597112,\n",
       " 0.024410153,\n",
       " 0.0137018785,\n",
       " 0.030860702,\n",
       " 0.004143225,\n",
       " 0.01819401,\n",
       " 0.034585837,\n",
       " 0.10234647,\n",
       " 0.48714632,\n",
       " 0.54876035,\n",
       " 0.6927431,\n",
       " 0.63095033,\n",
       " 0.20783673,\n",
       " 0.50892603,\n",
       " 0.5016808,\n",
       " 0.17521697,\n",
       " 0.2966614,\n",
       " 0.035653647,\n",
       " 0.10146447,\n",
       " 0.06098053,\n",
       " 0.025383731,\n",
       " 0.050029203,\n",
       " 0.033900023,\n",
       " 0.02948829,\n",
       " 0.02524016,\n",
       " 0.2554213,\n",
       " 0.065982714,\n",
       " 0.12964605,\n",
       " 0.07168174,\n",
       " 0.117512085,\n",
       " 0.37577638,\n",
       " 0.2847957,\n",
       " 0.08192286,\n",
       " 0.16841483,\n",
       " 0.34808394,\n",
       " 0.117074974,\n",
       " 0.092355095,\n",
       " 0.41961545,\n",
       " 0.2869163,\n",
       " 0.08735709,\n",
       " 0.051440142,\n",
       " 0.41527566,\n",
       " 0.061853126,\n",
       " 0.026662515,\n",
       " 0.2514276,\n",
       " 0.11146276,\n",
       " 0.0792883,\n",
       " 0.0940805,\n",
       " 0.07667171,\n",
       " 0.0443941,\n",
       " 0.10332675,\n",
       " 0.022131734,\n",
       " 0.04114997,\n",
       " 0.061035477,\n",
       " 0.045100722,\n",
       " 0.16869944,\n",
       " 0.11780121,\n",
       " 0.17043278,\n",
       " 0.10754599,\n",
       " 0.13478868,\n",
       " 0.09103511,\n",
       " 0.15433025,\n",
       " 0.2007033,\n",
       " 0.16281554,\n",
       " 0.08404788,\n",
       " 0.13168171,\n",
       " 0.22674409,\n",
       " 0.1423343,\n",
       " 0.077279374,\n",
       " 0.1453832,\n",
       " 0.15022862,\n",
       " 0.06174141,\n",
       " 0.08809026,\n",
       " 0.14175077,\n",
       " 0.062240984,\n",
       " 0.086988285,\n",
       " 0.08471841,\n",
       " 0.0418791,\n",
       " 0.09560993,\n",
       " 0.1602489,\n",
       " 0.08550365,\n",
       " 0.051413953,\n",
       " 0.14546984,\n",
       " 0.17536771,\n",
       " 0.18206711,\n",
       " 0.21278283,\n",
       " 0.12854847,\n",
       " 0.038414102,\n",
       " 0.06719196,\n",
       " 0.054634396,\n",
       " 0.07042518,\n",
       " 0.07930712,\n",
       " 0.032924555,\n",
       " 0.03894344,\n",
       " 0.08894328,\n",
       " 0.14483167,\n",
       " 0.079703145,\n",
       " 0.04668148,\n",
       " 0.06638099,\n",
       " 0.11285635,\n",
       " 0.07521876,\n",
       " 0.07684169,\n",
       " 0.061386757,\n",
       " 0.05304144,\n",
       " 0.066168584,\n",
       " 0.04888676,\n",
       " 0.046184264,\n",
       " 0.08612315,\n",
       " 0.12004393,\n",
       " 0.08922002,\n",
       " 0.07812654,\n",
       " 0.10473677,\n",
       " 0.062574625,\n",
       " 0.041646555,\n",
       " 0.01830156,\n",
       " 0.12365736,\n",
       " 0.063464195,\n",
       " 0.1296125,\n",
       " 0.18092436,\n",
       " 0.18444844,\n",
       " 0.15179649,\n",
       " 0.16743231,\n",
       " 0.26789257,\n",
       " 0.27047107,\n",
       " 0.29816067,\n",
       " 0.25124207,\n",
       " 0.32918042,\n",
       " 0.17020342,\n",
       " 0.06514749,\n",
       " 0.02548359,\n",
       " 0.046917565,\n",
       " 0.10656153,\n",
       " 0.024811056,\n",
       " 0.06618294,\n",
       " 0.062546335,\n",
       " 0.029516477,\n",
       " 0.024552967,\n",
       " 0.08573596,\n",
       " 0.058930553,\n",
       " 0.15749735,\n",
       " 0.3059932,\n",
       " 0.16780037,\n",
       " 0.25654858,\n",
       " 0.20892172,\n",
       " 0.14361237,\n",
       " 0.25789258,\n",
       " 0.2218519,\n",
       " 0.19997686,\n",
       " 0.25058928,\n",
       " 0.051910035,\n",
       " 0.073405534,\n",
       " 0.11258814,\n",
       " 0.036385585,\n",
       " 0.0332498,\n",
       " 0.13307337,\n",
       " 0.06337933,\n",
       " 0.09471707,\n",
       " 0.17868519,\n",
       " 0.21682489,\n",
       " 0.07862687,\n",
       " 0.23289031,\n",
       " 0.13253345,\n",
       " 0.13177446,\n",
       " 0.20799685,\n",
       " 0.08230468,\n",
       " 0.065560326,\n",
       " 0.17913364,\n",
       " 0.08801917,\n",
       " 0.06270068,\n",
       " 0.017724747,\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(label, pred)\n",
    "auroc = metrics.roc_auc_score(label, pred)\n",
    "\n",
    "auprc =  average_precision_score(label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7973225602451445, 0.42130765553807925)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc,auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(pred)):\n",
    "            if (pred[i] > 0.18):\n",
    "                pred[i] = 1\n",
    "            else:\n",
    "                pred[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7405672651574291,\n",
       " 0.6857295563869589,\n",
       " 0.3140912651782217,\n",
       " 0.4308405252023238,\n",
       " 0.3290373905451128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1 = accuracy_score(label, pred, sample_weight=None)\n",
    "# spec1 = spec1 + (cm1[0, 0]) / (cm1[0, 0] + cm1[0, 1])\n",
    "recall = recall_score(label, pred, sample_weight=None)\n",
    "prec1 = precision_score(label, pred, sample_weight=None)\n",
    "f1 = f1_score(label, pred)\n",
    "mcc = matthews_corrcoef(label, pred)\n",
    "acc1,recall,prec1,f1,mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for name,param_tensor in model.named_parameters():\n",
    "#     print(name,param_tensor.detach().numpy())\n",
    "dic={}\n",
    "for name,param_tensor in model.named_parameters():\n",
    "    dic[name]=param_tensor.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(dic,open('weight.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_gcn.convs.0.weight\n",
      "deep_gcn.convs.1.weight\n",
      "deep_gcn.convs.2.weight\n",
      "deep_gcn.convs.3.weight\n",
      "deep_gcn.convs.4.weight\n",
      "deep_gcn.convs.5.weight\n",
      "deep_gcn.convs.6.weight\n",
      "deep_gcn.convs.7.weight\n",
      "deep_gcn.fcs.0.weight\n",
      "deep_gcn.fcs.0.bias\n",
      "deep_gcn.fcs.1.weight\n",
      "deep_gcn.fcs.1.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['deep_gcn.convs.0.weight', 'deep_gcn.convs.1.weight', 'deep_gcn.convs.2.weight', 'deep_gcn.convs.3.weight', 'deep_gcn.convs.4.weight', 'deep_gcn.convs.5.weight', 'deep_gcn.convs.6.weight', 'deep_gcn.convs.7.weight', 'deep_gcn.fcs.0.weight', 'deep_gcn.fcs.0.bias', 'deep_gcn.fcs.1.weight', 'deep_gcn.fcs.1.bias'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, value in model.state_dict().items():\n",
    "    print(key)\n",
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "a=torch.zeros((2,100,10))\n",
    "n=nn.Linear(10,12)\n",
    "n(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 1], edge_index=[2, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
